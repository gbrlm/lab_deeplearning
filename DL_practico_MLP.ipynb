{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_practico_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6hmQbnl_Ip_",
        "outputId": "70805225-49f5-48a6-dc0b-5e27032a37e1"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpvMo3dF-N7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ec38c3-6d03-40d7-be52-068df53bdd0b"
      },
      "source": [
        "%%bash\n",
        "mkdir -p data\n",
        "\n",
        "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/meli-challenge-2019.tar.bz2 -o ./data/meli-challenge-2019.tar.bz2\n",
        "tar jxvf ./data/meli-challenge-2019.tar.bz2 -C ./data/\n",
        "\n",
        "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/SBW-vectors-300-min5.txt.gz -o ./data/SBW-vectors-300-min5.txt.gz\n",
        "\n",
        "pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "pip install gensim mlflow tqdm\n",
        "pip install pyngrok --quiet\n",
        "\n",
        "# Be sure the correct nvcc is in the path with the correct pytorch installation\n",
        "export CUDA_HOME=/opt/cuda/11.0\n",
        "export PATH=$CUDA_HOME/bin:$PATH\n",
        "export CUDA_VISIBLE_DEVICES=0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meli-challenge-2019/\n",
            "meli-challenge-2019/spanish.test.jsonl.gz\n",
            "meli-challenge-2019/portuguese.validation.jsonl.gz\n",
            "meli-challenge-2019/portuguese.train.jsonl.gz\n",
            "meli-challenge-2019/spanish.train.jsonl.gz\n",
            "meli-challenge-2019/spanish_token_to_index.json.gz\n",
            "meli-challenge-2019/portuguese_token_to_index.json.gz\n",
            "meli-challenge-2019/spanish.validation.jsonl.gz\n",
            "meli-challenge-2019/portuguese.test.jsonl.gz\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "Collecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting mlflow\n",
            "  Downloading https://files.pythonhosted.org/packages/58/dc/b45061f1cde42465f8ac1ebd86db3253a0e155619929bf1d6de271317c08/mlflow-1.14.1-py3-none-any.whl (14.2MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/d5/8a046d683c2cc084b6a502812827ede69b1064f95d93f94b83f809b21723/prometheus_flask_exporter-0.18.1.tar.gz\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "Collecting gunicorn; platform_system != \"Windows\"\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.12.4)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/af/631375abc29e59cedfa4467a5f7755503ba19898890751e1f2636ef02f92/databricks-cli-0.14.3.tar.gz (54kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.23)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/33/80e0d4f60e84a1ddd9a03f340be1065a2a363c47ce65c4bd3bae65ce9631/websocket_client-0.58.0-py2.py3-none-any.whl (61kB)\n",
            "Collecting Mako\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow) (2.8.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (54.1.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic<=1.4.1->mlflow) (1.1.1)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: prometheus-flask-exporter, alembic, databricks-cli\n",
            "  Building wheel for prometheus-flask-exporter (setup.py): started\n",
            "  Building wheel for prometheus-flask-exporter (setup.py): finished with status 'done'\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-cp37-none-any.whl size=17159 sha256=6b5b3ba35df52b71ce6182102768060b0324a169c84b441d3631e0eb7687d8c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/1f/b8/66bd9bc3a9d6c6987ff6c4dfeb6f1fe97b5a0e5ed5849c0437\n",
            "  Building wheel for alembic (setup.py): started\n",
            "  Building wheel for alembic (setup.py): finished with status 'done'\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=2c129d894ade53e48bab875cdaa70f0ee640e332c460782724be0b07748c57ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "  Building wheel for databricks-cli (setup.py): started\n",
            "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-cp37-none-any.whl size=100557 sha256=e28854a1b7adccf1f1c3be23a9b0dbab7631c38d9fe2765bd167624afe730bba\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/24/f3/34d8e3964dac4ba849d844273c49a679111b00d5799ebb934a\n",
            "Successfully built prometheus-flask-exporter alembic databricks-cli\n",
            "Installing collected packages: prometheus-flask-exporter, websocket-client, docker, Mako, python-editor, alembic, gunicorn, databricks-cli, querystring-parser, smmap, gitdb, gitpython, mlflow\n",
            "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.14.3 docker-4.4.4 gitdb-4.0.5 gitpython-3.1.14 gunicorn-20.0.4 mlflow-1.14.1 prometheus-flask-exporter-0.18.1 python-editor-1.0.4 querystring-parser-1.2.4 smmap-3.0.5 websocket-client-0.58.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  945M    0 81920    0     0  50319      0  5:28:15  0:00:01  5:28:14 50288\r  0  945M    0 6336k    0     0  2467k      0  0:06:32  0:00:02  0:06:30 2466k\r  2  945M    2 24.5M    0     0  7076k      0  0:02:16  0:00:03  0:02:13 7076k\r  4  945M    4 42.0M    0     0  9469k      0  0:01:42  0:00:04  0:01:38 9467k\r  6  945M    6 60.8M    0     0  10.9M      0  0:01:26  0:00:05  0:01:21 12.4M\r  8  945M    8 78.7M    0     0  12.0M      0  0:01:18  0:00:06  0:01:12 15.9M\r 10  945M   10 96.4M    0     0  12.7M      0  0:01:14  0:00:07  0:01:07 18.0M\r 12  945M   12  114M    0     0  13.3M      0  0:01:10  0:00:08  0:01:02 17.9M\r 14  945M   14  132M    0     0  13.9M      0  0:01:07  0:00:09  0:00:58 18.1M\r 15  945M   15  151M    0     0  14.3M      0  0:01:05  0:00:10  0:00:55 18.0M\r 17  945M   17  168M    0     0  14.6M      0  0:01:04  0:00:11  0:00:53 18.0M\r 19  945M   19  186M    0     0  14.8M      0  0:01:03  0:00:12  0:00:51 18.0M\r 21  945M   21  204M    0     0  15.1M      0  0:01:02  0:00:13  0:00:49 18.0M\r 23  945M   23  222M    0     0  15.2M      0  0:01:01  0:00:14  0:00:47 17.9M\r 25  945M   25  241M    0     0  15.5M      0  0:01:00  0:00:15  0:00:45 17.9M\r 27  945M   27  259M    0     0  15.6M      0  0:01:00  0:00:16  0:00:44 18.1M\r 29  945M   29  277M    0     0  15.8M      0  0:00:59  0:00:17  0:00:42 18.1M\r 31  945M   31  295M    0     0  15.9M      0  0:00:59  0:00:18  0:00:41 18.2M\r 33  945M   33  313M    0     0  16.0M      0  0:00:58  0:00:19  0:00:39 18.2M\r 35  945M   35  331M    0     0  16.1M      0  0:00:58  0:00:20  0:00:38 18.0M\r 36  945M   36  349M    0     0  16.2M      0  0:00:58  0:00:21  0:00:37 17.9M\r 38  945M   38  367M    0     0  16.2M      0  0:00:57  0:00:22  0:00:35 17.9M\r 40  945M   40  385M    0     0  16.3M      0  0:00:57  0:00:23  0:00:34 18.0M\r 42  945M   42  404M    0     0  16.4M      0  0:00:57  0:00:24  0:00:33 18.0M\r 44  945M   44  421M    0     0  16.5M      0  0:00:57  0:00:25  0:00:32 18.0M\r 46  945M   46  439M    0     0  16.5M      0  0:00:57  0:00:26  0:00:31 17.9M\r 48  945M   48  457M    0     0  16.6M      0  0:00:56  0:00:27  0:00:29 18.0M\r 50  945M   50  475M    0     0  16.6M      0  0:00:56  0:00:28  0:00:28 18.0M\r 52  945M   52  493M    0     0  16.7M      0  0:00:56  0:00:29  0:00:27 17.9M\r 54  945M   54  512M    0     0  16.7M      0  0:00:56  0:00:30  0:00:26 18.0M\r 56  945M   56  530M    0     0  16.8M      0  0:00:56  0:00:31  0:00:25 18.2M\r 58  945M   58  548M    0     0  16.8M      0  0:00:56  0:00:32  0:00:24 18.1M\r 59  945M   59  566M    0     0  16.8M      0  0:00:55  0:00:33  0:00:22 18.2M\r 61  945M   61  584M    0     0  16.9M      0  0:00:55  0:00:34  0:00:21 18.1M\r 63  945M   63  602M    0     0  16.9M      0  0:00:55  0:00:35  0:00:20 18.0M\r 65  945M   65  620M    0     0  16.9M      0  0:00:55  0:00:36  0:00:19 17.9M\r 67  945M   67  638M    0     0  16.9M      0  0:00:55  0:00:37  0:00:18 17.9M\r 69  945M   69  656M    0     0  17.0M      0  0:00:55  0:00:38  0:00:17 17.8M\r 71  945M   71  674M    0     0  17.0M      0  0:00:55  0:00:39  0:00:16 18.0M\r 73  945M   73  692M    0     0  17.0M      0  0:00:55  0:00:40  0:00:15 18.0M\r 75  945M   75  710M    0     0  17.1M      0  0:00:55  0:00:41  0:00:14 18.1M\r 77  945M   77  728M    0     0  17.1M      0  0:00:55  0:00:42  0:00:13 18.0M\r 79  945M   79  746M    0     0  17.1M      0  0:00:55  0:00:43  0:00:12 18.1M\r 80  945M   80  764M    0     0  17.1M      0  0:00:55  0:00:44  0:00:11 18.0M\r 82  945M   82  782M    0     0  17.1M      0  0:00:55  0:00:45  0:00:10 17.9M\r 84  945M   84  800M    0     0  17.1M      0  0:00:54  0:00:46  0:00:08 17.9M\r 86  945M   86  819M    0     0  17.2M      0  0:00:54  0:00:47  0:00:07 18.1M\r 88  945M   88  835M    0     0  17.2M      0  0:00:54  0:00:48  0:00:06 17.6M\r 90  945M   90  852M    0     0  17.2M      0  0:00:54  0:00:49  0:00:05 17.5M\r 92  945M   92  870M    0     0  17.2M      0  0:00:54  0:00:50  0:00:04 17.6M\r 94  945M   94  888M    0     0  17.2M      0  0:00:54  0:00:51  0:00:03 17.6M\r 95  945M   95  906M    0     0  17.2M      0  0:00:54  0:00:52  0:00:02 17.4M\r 97  945M   97  924M    0     0  17.2M      0  0:00:54  0:00:53  0:00:01 17.8M\r 99  945M   99  943M    0     0  17.2M      0  0:00:54  0:00:54 --:--:-- 18.1M\r100  945M  100  945M    0     0  17.2M      0  0:00:54  0:00:54 --:--:-- 18.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  973M    0  768k    0     0   364k      0  0:45:31  0:00:02  0:45:29  364k\r  1  973M    1 14.2M    0     0  4915k      0  0:03:22  0:00:02  0:03:20 4913k\r  3  973M    3 31.7M    0     0  8189k      0  0:02:01  0:00:03  0:01:58 8187k\r  5  973M    5 49.8M    0     0  10.0M      0  0:01:37  0:00:04  0:01:33 10.0M\r  6  973M    6 67.2M    0     0  11.2M      0  0:01:26  0:00:05  0:01:21 13.5M\r  8  973M    8 84.9M    0     0  12.1M      0  0:01:20  0:00:06  0:01:14 17.2M\r 10  973M   10  102M    0     0  12.8M      0  0:01:15  0:00:07  0:01:08 17.6M\r 12  973M   12  120M    0     0  13.4M      0  0:01:12  0:00:08  0:01:04 17.8M\r 14  973M   14  138M    0     0  13.9M      0  0:01:09  0:00:09  0:01:00 17.7M\r 16  973M   16  156M    0     0  14.2M      0  0:01:08  0:00:10  0:00:58 17.8M\r 17  973M   17  173M    0     0  14.5M      0  0:01:07  0:00:11  0:00:56 17.7M\r 19  973M   19  191M    0     0  14.7M      0  0:01:06  0:00:12  0:00:54 17.6M\r 21  973M   21  209M    0     0  14.9M      0  0:01:05  0:00:13  0:00:52 17.6M\r 23  973M   23  226M    0     0  15.1M      0  0:01:04  0:00:14  0:00:50 17.6M\r 25  973M   25  244M    0     0  15.3M      0  0:01:03  0:00:15  0:00:48 17.6M\r 26  973M   26  262M    0     0  15.4M      0  0:01:02  0:00:16  0:00:46 17.8M\r 28  973M   28  280M    0     0  15.5M      0  0:01:02  0:00:17  0:00:45 17.8M\r 30  973M   30  297M    0     0  15.6M      0  0:01:02  0:00:18  0:00:44 17.6M\r 32  973M   32  315M    0     0  15.7M      0  0:01:01  0:00:19  0:00:42 17.6M\r 34  973M   34  332M    0     0  15.8M      0  0:01:01  0:00:20  0:00:41 17.7M\r 36  973M   36  351M    0     0  15.9M      0  0:01:00  0:00:21  0:00:39 17.6M\r 37  973M   37  368M    0     0  16.0M      0  0:01:00  0:00:22  0:00:38 17.5M\r 39  973M   39  385M    0     0  16.0M      0  0:01:00  0:00:23  0:00:37 17.6M\r 41  973M   41  403M    0     0  16.1M      0  0:01:00  0:00:24  0:00:36 17.7M\r 43  973M   43  422M    0     0  16.2M      0  0:00:59  0:00:25  0:00:34 17.8M\r 45  973M   45  439M    0     0  16.2M      0  0:00:59  0:00:26  0:00:33 17.6M\r 47  973M   47  457M    0     0  16.3M      0  0:00:59  0:00:27  0:00:32 17.8M\r 48  973M   48  475M    0     0  16.4M      0  0:00:59  0:00:28  0:00:31 17.9M\r 50  973M   50  493M    0     0  16.4M      0  0:00:59  0:00:29  0:00:30 17.9M\r 52  973M   52  510M    0     0  16.4M      0  0:00:59  0:00:30  0:00:29 17.7M\r 54  973M   54  529M    0     0  16.5M      0  0:00:58  0:00:31  0:00:27 17.9M\r 56  973M   56  546M    0     0  16.5M      0  0:00:58  0:00:32  0:00:26 17.8M\r 58  973M   58  564M    0     0  16.6M      0  0:00:58  0:00:33  0:00:25 17.8M\r 59  973M   59  582M    0     0  16.6M      0  0:00:58  0:00:34  0:00:24 17.8M\r 61  973M   61  600M    0     0  16.6M      0  0:00:58  0:00:35  0:00:23 17.9M\r 63  973M   63  617M    0     0  16.7M      0  0:00:58  0:00:36  0:00:22 17.7M\r 65  973M   65  636M    0     0  16.7M      0  0:00:58  0:00:37  0:00:21 17.8M\r 67  973M   67  653M    0     0  16.7M      0  0:00:58  0:00:38  0:00:20 17.7M\r 69  973M   69  671M    0     0  16.8M      0  0:00:57  0:00:39  0:00:18 17.8M\r 70  973M   70  689M    0     0  16.8M      0  0:00:57  0:00:40  0:00:17 17.9M\r 72  973M   72  706M    0     0  16.8M      0  0:00:57  0:00:41  0:00:16 17.6M\r 74  973M   74  724M    0     0  16.8M      0  0:00:57  0:00:42  0:00:15 17.6M\r 76  973M   76  742M    0     0  16.8M      0  0:00:57  0:00:43  0:00:14 17.6M\r 78  973M   78  760M    0     0  16.9M      0  0:00:57  0:00:44  0:00:13 17.7M\r 79  973M   79  778M    0     0  16.9M      0  0:00:57  0:00:45  0:00:12 17.6M\r 81  973M   81  795M    0     0  16.9M      0  0:00:57  0:00:46  0:00:11 17.9M\r 83  973M   83  813M    0     0  16.9M      0  0:00:57  0:00:47  0:00:10 17.8M\r 85  973M   85  831M    0     0  16.9M      0  0:00:57  0:00:48  0:00:09 17.9M\r 87  973M   87  849M    0     0  16.9M      0  0:00:57  0:00:49  0:00:08 17.7M\r 89  973M   89  867M    0     0  17.0M      0  0:00:57  0:00:50  0:00:07 17.8M\r 90  973M   90  885M    0     0  17.0M      0  0:00:57  0:00:51  0:00:06 17.8M\r 92  973M   92  902M    0     0  17.0M      0  0:00:57  0:00:52  0:00:05 17.7M\r 94  973M   94  920M    0     0  17.0M      0  0:00:57  0:00:53  0:00:04 17.7M\r 96  973M   96  938M    0     0  17.0M      0  0:00:56  0:00:54  0:00:02 17.8M\r 98  973M   98  956M    0     0  17.0M      0  0:00:56  0:00:55  0:00:01 17.9M\r100  973M  100  973M    0     0  17.1M      0  0:00:56  0:00:56 --:--:-- 17.8M\n",
            "ERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.7.1+cu101 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvYnr-4JehYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319d07b4-0fab-47fa-9fea-66ef186fdbc2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhSJpxLVxqkm"
      },
      "source": [
        "import argparse\n",
        "import gzip\n",
        "import json\n",
        "import logging\n",
        "import mlflow\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm, trange\n",
        "from IPython import get_ipython\n",
        "from pyngrok import ngrok\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s: %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSZm2RKlx20b"
      },
      "source": [
        "class MeliChallengeDataset(IterableDataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path,\n",
        "                 random_buffer_size=2048):\n",
        "        assert random_buffer_size > 0\n",
        "        self.dataset_path = dataset_path\n",
        "        self.random_buffer_size = random_buffer_size\n",
        "\n",
        "        with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
        "            item = json.loads(next(dataset).strip())\n",
        "            self.n_labels = item[\"n_labels\"]\n",
        "            self.dataset_size = item[\"size\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        try:\n",
        "            with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
        "                shuffle_buffer = []\n",
        "\n",
        "                for line in dataset:\n",
        "                    item = json.loads(line.strip())\n",
        "                    item = {\n",
        "                        \"data\": item[\"data\"],\n",
        "                        \"target\": item[\"target\"]\n",
        "                    }\n",
        "\n",
        "                    if self.random_buffer_size == 1:\n",
        "                        yield item\n",
        "                    else:\n",
        "                        shuffle_buffer.append(item)\n",
        "\n",
        "                        if len(shuffle_buffer) == self.random_buffer_size:\n",
        "                            random.shuffle(shuffle_buffer)\n",
        "                            for item in shuffle_buffer:\n",
        "                                yield item\n",
        "                            shuffle_buffer = []\n",
        "\n",
        "                if len(shuffle_buffer) > 0:\n",
        "                    random.shuffle(shuffle_buffer)\n",
        "                    for item in shuffle_buffer:\n",
        "                        yield item\n",
        "        except GeneratorExit:\n",
        "            return"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_X0XsyaIdoR"
      },
      "source": [
        "common_params = {\n",
        "    'train_data': \"./data/meli-challenge-2019/spanish.train.jsonl.gz\",\n",
        "    'token_to_index': \"./data/meli-challenge-2019/spanish_token_to_index.json.gz\",\n",
        "    'pretrained_embeddings': \"./data/SBW-vectors-300-min5.txt.gz\",\n",
        "    'language': \"spanish\",\n",
        "    'test_data': None, # \"./data/meli-challenge-2019/spanish.test.jsonl.gz\",\n",
        "    'validation_data': \"./data/meli-challenge-2019/spanish.validation.jsonl.gz\",\n",
        "}\n",
        "\n",
        "parametrizable_params = [\n",
        "  {\n",
        "    'embeddings_size': 300,\n",
        "    'hidden_layers': [512, 256, 128], \n",
        "    'dropout': 0.25,\n",
        "    'epochs': 3,\n",
        "    'act_fun': 1,\n",
        "  },\n",
        "\n",
        "  {\n",
        "    'embeddings_size': 300,\n",
        "    'hidden_layers': [256, 128], \n",
        "    'dropout': 0.25,\n",
        "    'epochs': 3,\n",
        "    'act_fun': 2,\n",
        "  },\n",
        "  # {\n",
        "  #   'embeddings_size': 300,\n",
        "  #   'hidden_layers': [1024, 512, 256, 128], \n",
        "  #   'dropout': 0.25,\n",
        "  #   'epochs': 3,\n",
        "  #   'act_fun': 1,\n",
        "  # },\n",
        "  # {\n",
        "  #   'embeddings_size': 300,\n",
        "  #   'hidden_layers': [1024, 512, 256, 128], \n",
        "  #   'dropout': 0.25,\n",
        "  #   'epochs': 3,\n",
        "  #   'act_fun': 2,\n",
        "  # },\n",
        "  # {\n",
        "  #   'embeddings_size': 300,\n",
        "  #   'hidden_layers': [1024, 512, 256, 128], \n",
        "  #   'dropout': 0.2,\n",
        "  #   'epochs': 3,\n",
        "  #   'act_fun': 1,\n",
        "  # },\n",
        "  # {\n",
        "  #   'embeddings_size': 300,\n",
        "  #   'hidden_layers': [1024, 512, 256, 128], \n",
        "  #   'dropout': 0.2,\n",
        "  #   'epochs': 3,\n",
        "  #   'act_fun': 2,\n",
        "  # }\n",
        "]\n",
        "\n",
        "# cambiar: hidden_layers = (512, 256, 128) ; (1024, 512, 256, 128)\n",
        "# cambiar: dropout = 0.1; 0.2; 0.3; 0.4\n",
        "# cambiar: epochs = 5, 7, 10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMuOEZfNx3un"
      },
      "source": [
        "class PadSequences:\n",
        "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
        "        assert max_length is None or min_length <= max_length\n",
        "        self.pad_value = pad_value\n",
        "        self.max_length = max_length\n",
        "        self.min_length = min_length\n",
        "\n",
        "    def __call__(self, items):\n",
        "        data = [item[\"data\"] for item in items]\n",
        "        target = [item[\"target\"] for item in items]\n",
        "        seq_lengths = [len(d) for d in data]\n",
        "\n",
        "        if self.max_length:\n",
        "            max_length = self.max_length\n",
        "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
        "        else:\n",
        "            max_length = max(self.min_length, max(seq_lengths))\n",
        "\n",
        "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
        "                for d, l in zip(data, seq_lengths)]\n",
        "\n",
        "        return {\n",
        "            \"data\": torch.LongTensor(data),\n",
        "            \"target\": torch.LongTensor(target)\n",
        "        }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCTJt0yGx-G8"
      },
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    # Pytorch Module\n",
        "    # __init__:defines the structure of the network\n",
        "    def __init__(self,\n",
        "                 pretrained_embeddings_path,\n",
        "                 token_to_index,\n",
        "                 n_labels,\n",
        "                 hidden_layers=[256, 128],\n",
        "                 dropout=0.3,\n",
        "                 vector_size=300,\n",
        "                 act_fun=1,\n",
        "                 freeze_embedings=True):\n",
        "        super().__init__()\n",
        "        with gzip.open(token_to_index, \"rt\") as fh:\n",
        "            token_to_index = json.load(fh)\n",
        "        embeddings_matrix = torch.randn(len(token_to_index), vector_size)\n",
        "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
        "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
        "            next(fh)\n",
        "            for line in fh:\n",
        "                word, vector = line.strip().split(None, 1)\n",
        "                if word in token_to_index:\n",
        "                    embeddings_matrix[token_to_index[word]] =\\\n",
        "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
        "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
        "                                                       freeze=freeze_embedings,\n",
        "                                                       padding_idx=0)\n",
        "        ## Hidden layers definitions\n",
        "        ############################\n",
        "        ## https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "        self.hidden_layers = [\n",
        "            nn.Linear(vector_size, hidden_layers[0]) # first layer\n",
        "        ]\n",
        "        for input_size, output_size in zip(hidden_layers[:-1], hidden_layers[1:]):\n",
        "            self.hidden_layers.append(\n",
        "                nn.Linear(input_size, output_size) # intermediate layers if hidden_layers´s size > 2\n",
        "            )\n",
        "        self.dropout = dropout # percentage of disabled neurons\n",
        "        self.hidden_layers = nn.ModuleList(self.hidden_layers) #  last layer\n",
        "        self.output = nn.Linear(hidden_layers[-1], n_labels) \n",
        "        self.vector_size = vector_size\n",
        "        self.act_fun = act_fun\n",
        "\n",
        "\n",
        "    ############################\n",
        "    # forward: defines how the network layers interact\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        for layer in self.hidden_layers:\n",
        "            if self.act_fun == 1:\n",
        "                x = F.relu(layer(x))\n",
        "            if self.act_fun == 2:\n",
        "                x = F.celu(layer(x))\n",
        "            if self.dropout:\n",
        "                x = F.dropout(x, self.dropout)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ8Hq14oBA41"
      },
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self, \n",
        "#                  pretrained_embeddings_path, \n",
        "#                  token_to_index,             \n",
        "#                  n_labels,\n",
        "#                  vector_size,\n",
        "#                  FILTERS_COUNT,\n",
        "#                  FILTERS_LENGTH,\n",
        "#                  act_fun,\n",
        "#                  freeze_embedings):\n",
        "#         super().__init__()\n",
        "#         with gzip.open(token_to_index, \"rt\") as fh:\n",
        "#             token_to_index = json.load(fh)\n",
        "#         embeddings_matrix = torch.randn(len(token_to_index), vector_size)\n",
        "#         embeddings_matrix[0] = torch.zeros(vector_size)\n",
        "#         with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
        "#             next(fh)\n",
        "#             for line in fh:\n",
        "#                 word, vector = line.strip().split(None, 1)\n",
        "#                 if word in token_to_index:\n",
        "#                     embeddings_matrix[token_to_index[word]] =\\\n",
        "#                         torch.FloatTensor([float(n) for n in vector.split()])\n",
        "#         self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
        "#                                                        freeze=freeze_embedings,\n",
        "#                                                        padding_idx=0)\n",
        "#         self.FILTERS_COUNT = FILTERS_COUNT\n",
        "#         self.FILTERS_LENGTH = FILTERS_LENGTH\n",
        "#         self.act_fun = act_fun\n",
        "#         self.convs = []\n",
        "#         for filter_lenght in self.FILTERS_LENGTH:\n",
        "#             self.convs.append(\n",
        "#                 nn.Conv1d(vector_size, self.FILTERS_COUNT, filter_lenght)\n",
        "#             )\n",
        "#         self.convs = nn.ModuleList(self.convs)\n",
        "#         self.fc = nn.Linear(self.FILTERS_COUNT * len(self.FILTERS_LENGTH), 128)\n",
        "#         self.output = nn.Linear(128, n_labels)\n",
        "#         self.vector_size = vector_size\n",
        "    \n",
        "#     @staticmethod\n",
        "#     def conv_global_max_pool(x, conv):\n",
        "#         return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = self.embeddings(x).transpose(1, 2)\n",
        "#         x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
        "#         x = torch.cat(x, dim=1)\n",
        "#         if self.act_fun == 1:\n",
        "#             x = F.relu(self.fc(x))\n",
        "#         if self.act_fun == 2:\n",
        "#             x = F.celu(self.fc(x))\n",
        "#         # cambiar: x = F.hardsigmoid(layer(x)); F.celu(layer(x)); ; F.leaky_relu(layer(x))         \n",
        "#         x = self.output(x)\n",
        "#         return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB6Tql2pd4zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1900da5b-e95d-4744-9bbd-d1eca10265ec"
      },
      "source": [
        "pad_sequences = PadSequences(\n",
        "    pad_value=0,\n",
        "    max_length=None,\n",
        "    min_length=1\n",
        ")\n",
        "\n",
        "logging.info(\"Building training dataset\")\n",
        "# An iterable Dataset.\n",
        "# All datasets that represent an iterable of data samples should subclass it. \n",
        "# Such form of datasets is particularly useful when data come from a stream.\n",
        "# All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.\n",
        "train_dataset = MeliChallengeDataset(\n",
        "    dataset_path=common_params.get('train_data'),\n",
        "    random_buffer_size=2048  # This can be a hypterparameter\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,              # dataset from which to load the data.\n",
        "    batch_size=128,             # This can be a hyperparameter # how many samples per batch to load (default: ``1``).\n",
        "    shuffle=False,              # set to ``True`` to have the data reshuffled at every epoch (default: ``False``).\n",
        "    collate_fn=pad_sequences,   # merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.\n",
        "    drop_last=False,             # set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. \n",
        "                                # If ``False`` and the size of dataset is not divisible by the batch size, then the last batch\n",
        "                                # will be smaller. (default: ``False``)\n",
        "    num_workers=2             # how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. (default: ``0``)\n",
        ")\n",
        "\n",
        "if common_params.get('validation_data'):\n",
        "     logging.info(\"Building validation dataset\")\n",
        "     validation_dataset = MeliChallengeDataset(\n",
        "         dataset_path=common_params.get('validation_data'),\n",
        "         random_buffer_size=1\n",
        "     )\n",
        "     validation_loader = DataLoader(\n",
        "         validation_dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=False,\n",
        "         collate_fn=pad_sequences,\n",
        "         drop_last=False\n",
        "     )\n",
        "else:\n",
        "     validation_dataset = None\n",
        "     validation_loader = None\n",
        "\n",
        "if common_params.get('test_data'):\n",
        "     logging.info(\"Building test dataset\")\n",
        "     test_dataset = MeliChallengeDataset(\n",
        "         dataset_path=common_params.get('test_data'),\n",
        "         random_buffer_size=1\n",
        "     )\n",
        "     test_loader = DataLoader(\n",
        "         test_dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=False,\n",
        "         collate_fn=pad_sequences,\n",
        "         drop_last=False\n",
        "     )\n",
        "else:\n",
        "    test_dataset = None\n",
        "    test_loader = None\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 23:17:49,688: INFO - Building training dataset\n",
            "2021-03-21 23:17:49,696: INFO - Building validation dataset\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqJXrlGQavLN"
      },
      "source": [
        "## Iterando params\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U46ly2RPVf0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e95d6f-9bd7-44da-efbb-a08100e5f672"
      },
      "source": [
        "for params in parametrizable_params:\n",
        "  mlflow.set_experiment(f\"diplodatos.{common_params.get('language')}\")\n",
        "  with mlflow.start_run():\n",
        "    logging.info(\"Starting experiment\")\n",
        "    # Log all relevent hyperparameters\n",
        "    mlflow.log_params({\n",
        "      \"model_type\": \"Multilayer Perceptron\",\n",
        "      \"embeddings\": common_params.get('pretrained_embeddings'),\n",
        "      **params\n",
        "    })\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    logging.info(\"Building classifier\")\n",
        "    model = MLPClassifier(\n",
        "        pretrained_embeddings_path=common_params.get('pretrained_embeddings'),\n",
        "        token_to_index=common_params.get('token_to_index'),\n",
        "        n_labels=train_dataset.n_labels,\n",
        "        hidden_layers=params.get('hidden_layers'),\n",
        "        dropout=params.get('dropout'),\n",
        "        vector_size=params.get('embeddings_size'),\n",
        "        act_fun=params.get('act_fun'),\n",
        "        freeze_embedings=True  # This can be a hyperparameter\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    # loss function\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    loss = nn.CrossEntropyLoss()        \n",
        "    # optimizer algorithm\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    # cambiar: lr; weight_decay; momentum\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,           # This can be a hyperparameter\n",
        "        weight_decay=1e-5  # This can be a hyperparameter # weight for L2 regularization\n",
        "        # momentum=        # This can be a hyperparameter\n",
        "    )\n",
        "\n",
        "    logging.info(\"Training classifier\")\n",
        "    for epoch in trange(params.get('epochs')):\n",
        "        model.train()\n",
        "        running_loss = []\n",
        "        for idx, batch in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
        "            # set to zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # get the inputs; data and target\n",
        "            data = batch[\"data\"].to(device)\n",
        "            target = batch[\"target\"].to(device)\n",
        "            # forward + backward + optimize\n",
        "            output = model(data) # MLPClassifier\n",
        "            loss_value = loss(output, target)\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            # statistics\n",
        "            running_loss.append(loss_value.item())\n",
        "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "\n",
        "        if validation_dataset:\n",
        "            logging.info(\"Evaluating model on validation\")\n",
        "            model.eval()\n",
        "            running_loss = []\n",
        "            targets = []\n",
        "            predictions = []\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(validation_loader, position=0, leave=True):\n",
        "                    data = batch[\"data\"].to(device)\n",
        "                    target = batch[\"target\"].to(device)\n",
        "                    output = model(data)\n",
        "                    running_loss.append(\n",
        "                        loss(output, target).item()\n",
        "                    )\n",
        "                    targets.extend(batch[\"target\"].numpy())\n",
        "                    predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "                mlflow.log_metric(\"validation_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "                mlflow.log_metric(\"validation_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
        "\n",
        "    if test_dataset:\n",
        "        logging.info(\"Evaluating model on test\")\n",
        "        model.eval()\n",
        "        running_loss = []\n",
        "        targets = []\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, position=0, leave=True):\n",
        "                data = batch[\"data\"].to(device)\n",
        "                target = batch[\"target\"].to(device)\n",
        "                output = model(data)\n",
        "                running_loss.append(\n",
        "                    loss(output, target).item()\n",
        "                )\n",
        "                targets.extend(batch[\"target\"].numpy())\n",
        "                predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "            mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "            mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 23:17:51,968: INFO - Starting experiment\n",
            "2021-03-21 23:17:52,037: INFO - Building classifier\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO: 'diplodatos.spanish' does not exist. Creating a new experiment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 23:18:23,917: INFO - Training classifier\n",
            "76490it [06:43, 189.42it/s]\n",
            "2021-03-21 23:25:07,734: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:19<00:00, 482.66it/s]\n",
            "76490it [06:56, 183.68it/s]\n",
            "2021-03-21 23:32:25,684: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:19<00:00, 487.08it/s]\n",
            "76490it [06:55, 183.97it/s]\n",
            "2021-03-21 23:39:42,972: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:19<00:00, 483.39it/s]\n",
            "100%|██████████| 3/3 [21:40<00:00, 433.56s/it]\n",
            "2021-03-21 23:40:04,632: INFO - Starting experiment\n",
            "2021-03-21 23:40:04,637: INFO - Building classifier\n",
            "2021-03-21 23:40:29,232: INFO - Training classifier\n",
            "76490it [06:18, 202.05it/s]\n",
            "2021-03-21 23:46:47,819: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:18<00:00, 505.82it/s]\n",
            "76490it [06:18, 202.30it/s]\n",
            "2021-03-21 23:53:26,710: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:18<00:00, 513.25it/s]\n",
            "76490it [06:15, 203.62it/s]\n",
            "2021-03-22 00:00:02,877: INFO - Evaluating model on validation\n",
            "100%|██████████| 9562/9562 [00:18<00:00, 507.81it/s]\n",
            "100%|██████████| 3/3 [19:54<00:00, 398.11s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNkOoav0w8eU"
      },
      "source": [
        "# for params in parametrizable_params:\n",
        "#   mlflow.set_experiment(f\"diplodatos.{common_params.get('language')}\")\n",
        "#   with mlflow.start_run():\n",
        "#     logging.info(\"Starting experiment\")\n",
        "#     # Log all relevent hyperparameters\n",
        "#     mlflow.log_params({\n",
        "#       \"model_type\": \"Convolutional Neural Network\",\n",
        "#       \"embeddings\": common_params.get('pretrained_embeddings'),\n",
        "#       **params\n",
        "#     })\n",
        "#     device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "#     logging.info(\"Building classifier\")\n",
        "#     model = CNN(\n",
        "#         pretrained_embeddings_path=common_params.get('pretrained_embeddings'),\n",
        "#         token_to_index=common_params.get('token_to_index'),\n",
        "#         n_labels=train_dataset.n_labels,\n",
        "#         vector_size=params.get('embeddings_size'),\n",
        "#         FILTERS_COUNT=params.get('FILTERS_COUNT'),\n",
        "#         FILTERS_LENGTH=params.get('FILTERS_LENGTH'),\n",
        "#         act_fun=params.get('act_fun'),\n",
        "#         freeze_embedings=True  # This can be a hyperparameter\n",
        "#     )\n",
        "     \n",
        "#     model = model.to(device)\n",
        "#     # loss function\n",
        "#     # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#     loss = nn.CrossEntropyLoss()        \n",
        "#     # optimizer algorithm\n",
        "#     # https://pytorch.org/docs/stable/optim.html\n",
        "#     # cambiar: lr; weight_decay; momentum\n",
        "#     optimizer = optim.Adam(\n",
        "#         model.parameters(),\n",
        "#         lr=1e-3,           # This can be a hyperparameter\n",
        "#         weight_decay=1e-5  # This can be a hyperparameter # weight for L2 regularization\n",
        "#         # momentum=        # This can be a hyperparameter\n",
        "#     )\n",
        "\n",
        "#     logging.info(\"Training classifier\")\n",
        "#     for epoch in trange(params.get('epochs')):\n",
        "#         model.train()\n",
        "#         running_loss = []\n",
        "#         for idx, batch in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
        "#             # set to zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # get the inputs; data and target\n",
        "#             data = batch[\"data\"].to(device)\n",
        "#             target = batch[\"target\"].to(device)\n",
        "#             # forward + backward + optimize\n",
        "#             output = model(data) # MLPClassifier\n",
        "#             loss_value = loss(output, target)\n",
        "#             loss_value.backward()\n",
        "#             optimizer.step()\n",
        "#             # statistics\n",
        "#             running_loss.append(loss_value.item())\n",
        "#         mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "\n",
        "#         if validation_dataset:\n",
        "#             logging.info(\"Evaluating model on validation\")\n",
        "#             model.eval()\n",
        "#             running_loss = []\n",
        "#             targets = []\n",
        "#             predictions = []\n",
        "#             with torch.no_grad():\n",
        "#                 for batch in tqdm(validation_loader, position=0, leave=True):\n",
        "#                     data = batch[\"data\"].to(device)\n",
        "#                     target = batch[\"target\"].to(device)\n",
        "#                     output = model(data)\n",
        "#                     running_loss.append(\n",
        "#                         loss(output, target).item()\n",
        "#                     )\n",
        "#                     targets.extend(batch[\"target\"].numpy())\n",
        "#                     predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "#                 mlflow.log_metric(\"validation_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "#                 mlflow.log_metric(\"validation_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
        "\n",
        "#     if test_dataset:\n",
        "#         logging.info(\"Evaluating model on test\")\n",
        "#         model.eval()\n",
        "#         running_loss = []\n",
        "#         targets = []\n",
        "#         predictions = []\n",
        "#         with torch.no_grad():\n",
        "#             for batch in tqdm(test_loader, position=0, leave=True):\n",
        "#                 data = batch[\"data\"].to(device)\n",
        "#                 target = batch[\"target\"].to(device)\n",
        "#                 output = model(data)\n",
        "#                 running_loss.append(\n",
        "#                     loss(output, target).item()\n",
        "#                 )\n",
        "#                 targets.extend(batch[\"target\"].numpy())\n",
        "#                 predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "#             mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "#             mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKw8EKRXgPzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1990ab5d-3591-47ae-a9d2-9a01b8477769"
      },
      "source": [
        "    # run tracking UI in the background\n",
        "    get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n",
        "\n",
        "\n",
        "    # create remote tunnel using ngrok.com to allow local port access\n",
        "    # borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "\n",
        "    # Terminate open tunnels if exist\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Setting the authtoken (optional)\n",
        "    # Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "    NGROK_AUTH_TOKEN = \"\"\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    # Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "    ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "    print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-22 00:00:30,704: INFO - Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.7/dist-packages/pyngrok/bin/ngrok\n",
            "2021-03-22 00:00:30,804: INFO - Opening tunnel named: http-5000-6a910677-6269-4ce6-9bc3-8d6f4e656ddd\n",
            "2021-03-22 00:00:30,900: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2021-03-22 00:00:30,902: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2021-03-22 00:00:30,903: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2021-03-22 00:00:30,906: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2021-03-22 00:00:30,990: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2021-03-22 00:00:30,991: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=\"client session established\" obj=csess id=b4a8805c27c9\n",
            "2021-03-22 00:00:31,006: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=start pg=/api/tunnels id=632f660cd8d82b46\n",
            "2021-03-22 00:00:31,014: INFO - t=2021-03-22T00:00:30+0000 lvl=info msg=end pg=/api/tunnels id=632f660cd8d82b46 status=200 dur=297.852µs\n",
            "2021-03-22 00:00:31,016: INFO - t=2021-03-22T00:00:31+0000 lvl=info msg=start pg=/api/tunnels id=77b7e82138e894da\n",
            "2021-03-22 00:00:31,017: INFO - t=2021-03-22T00:00:31+0000 lvl=info msg=end pg=/api/tunnels id=77b7e82138e894da status=200 dur=102.603µs\n",
            "2021-03-22 00:00:31,018: INFO - t=2021-03-22T00:00:31+0000 lvl=info msg=start pg=/api/tunnels id=71dcb82747045b16\n",
            "2021-03-22 00:00:31,025: INFO - t=2021-03-22T00:00:31+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-5000-6a910677-6269-4ce6-9bc3-8d6f4e656ddd addr=http://localhost:5000 url=https://6f6b22605d2f.ngrok.io\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://6f6b22605d2f.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-22 00:00:31,028: INFO - t=2021-03-22T00:00:31+0000 lvl=info msg=end pg=/api/tunnels id=71dcb82747045b16 status=201 dur=19.902363ms\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvCZfkCEgcIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12daca9c-1262-4915-9aab-bc25503dc6f7"
      },
      "source": [
        "!zip -r ./mlruns_mlp.zip ./mlruns\n",
        "from google.colab import files\n",
        "# files.download(\"./mlruns.zip\")\n",
        "!cp ./mlruns_mlp.zip ./drive/MyDrive"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: mlruns/ (stored 0%)\n",
            "  adding: mlruns/.trash/ (stored 0%)\n",
            "  adding: mlruns/1/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/metrics/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/metrics/validation_bacc (deflated 35%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/metrics/train_loss (deflated 32%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/metrics/validation_loss (deflated 35%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/artifacts/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/tags/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/tags/mlflow.source.type (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/tags/mlflow.user (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/tags/mlflow.source.name (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/meta.yaml (deflated 44%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/ (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/embeddings_size (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/epochs (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/model_type (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/act_fun (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/hidden_layers (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/embeddings (stored 0%)\n",
            "  adding: mlruns/1/69f1c2fbcdb54e108e898baa14069b5a/params/dropout (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/ (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/metrics/ (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/metrics/validation_bacc (deflated 34%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/metrics/train_loss (deflated 33%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/metrics/validation_loss (deflated 33%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/artifacts/ (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/tags/ (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/tags/mlflow.source.type (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/tags/mlflow.user (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/tags/mlflow.source.name (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/meta.yaml (deflated 45%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/ (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/embeddings_size (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/epochs (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/model_type (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/act_fun (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/hidden_layers (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/embeddings (stored 0%)\n",
            "  adding: mlruns/1/7b5706327ae949e3b8c98fff103022a8/params/dropout (stored 0%)\n",
            "  adding: mlruns/1/meta.yaml (deflated 14%)\n",
            "  adding: mlruns/0/ (stored 0%)\n",
            "  adding: mlruns/0/meta.yaml (deflated 11%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsoUsQ7i6eG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}