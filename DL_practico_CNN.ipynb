{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_practico_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6hmQbnl_Ip_",
        "outputId": "f21d446a-a28e-48ac-e64f-dfdaaa900ea4"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpvMo3dF-N7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba470688-6cb8-42a1-d610-954e3c9df68f"
      },
      "source": [
        "%%bash\n",
        "mkdir -p data\n",
        "\n",
        "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/meli-challenge-2019.tar.bz2 -o ./data/meli-challenge-2019.tar.bz2\n",
        "tar jxvf ./data/meli-challenge-2019.tar.bz2 -C ./data/\n",
        "\n",
        "curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/SBW-vectors-300-min5.txt.gz -o ./data/SBW-vectors-300-min5.txt.gz\n",
        "\n",
        "pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "pip install gensim mlflow tqdm\n",
        "pip install pyngrok --quiet\n",
        "\n",
        "# Be sure the correct nvcc is in the path with the correct pytorch installation\n",
        "export CUDA_HOME=/opt/cuda/11.0\n",
        "export PATH=$CUDA_HOME/bin:$PATH\n",
        "export CUDA_VISIBLE_DEVICES=0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meli-challenge-2019/\n",
            "meli-challenge-2019/spanish.test.jsonl.gz\n",
            "meli-challenge-2019/portuguese.validation.jsonl.gz\n",
            "meli-challenge-2019/portuguese.train.jsonl.gz\n",
            "meli-challenge-2019/spanish.train.jsonl.gz\n",
            "meli-challenge-2019/spanish_token_to_index.json.gz\n",
            "meli-challenge-2019/portuguese_token_to_index.json.gz\n",
            "meli-challenge-2019/spanish.validation.jsonl.gz\n",
            "meli-challenge-2019/portuguese.test.jsonl.gz\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "Collecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting mlflow\n",
            "  Downloading https://files.pythonhosted.org/packages/58/dc/b45061f1cde42465f8ac1ebd86db3253a0e155619929bf1d6de271317c08/mlflow-1.14.1-py3-none-any.whl (14.2MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "Collecting gunicorn; platform_system != \"Windows\"\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/af/631375abc29e59cedfa4467a5f7755503ba19898890751e1f2636ef02f92/databricks-cli-0.14.3.tar.gz (54kB)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.2)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.23)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.12.4)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.13)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/d5/8a046d683c2cc084b6a502812827ede69b1064f95d93f94b83f809b21723/prometheus_flask_exporter-0.18.1.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.1)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/33/80e0d4f60e84a1ddd9a03f340be1065a2a363c47ce65c4bd3bae65ce9631/websocket_client-0.58.0-py2.py3-none-any.whl (61kB)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (54.1.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2020.12.5)\n",
            "Collecting Mako\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: databricks-cli, alembic, prometheus-flask-exporter\n",
            "  Building wheel for databricks-cli (setup.py): started\n",
            "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-cp37-none-any.whl size=100557 sha256=b30c2c7e1955a4a1b1d6ef39f6df6294ba96a8d691299b957f9f1eaa6bb3bbf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/24/f3/34d8e3964dac4ba849d844273c49a679111b00d5799ebb934a\n",
            "  Building wheel for alembic (setup.py): started\n",
            "  Building wheel for alembic (setup.py): finished with status 'done'\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=e96c6d347e47f7c6821afbda4d2ce8576e2172acda71e36bf4c988759c87a2e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "  Building wheel for prometheus-flask-exporter (setup.py): started\n",
            "  Building wheel for prometheus-flask-exporter (setup.py): finished with status 'done'\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-cp37-none-any.whl size=17159 sha256=e65db49895cc7b16cca017104ef3a6beb0c30b54927b4872565e9bad2537057c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/1f/b8/66bd9bc3a9d6c6987ff6c4dfeb6f1fe97b5a0e5ed5849c0437\n",
            "Successfully built databricks-cli alembic prometheus-flask-exporter\n",
            "Installing collected packages: websocket-client, docker, gunicorn, databricks-cli, querystring-parser, Mako, python-editor, alembic, smmap, gitdb, gitpython, prometheus-flask-exporter, mlflow\n",
            "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.14.3 docker-4.4.4 gitdb-4.0.5 gitpython-3.1.14 gunicorn-20.0.4 mlflow-1.14.1 prometheus-flask-exporter-0.18.1 python-editor-1.0.4 querystring-parser-1.2.4 smmap-3.0.5 websocket-client-0.58.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  945M    0  160k    0     0  92669      0  2:58:14  0:00:01  2:58:13 92617\r  0  945M    0 2368k    0     0   864k      0  0:18:39  0:00:02  0:18:37  863k\r  0  945M    0 5088k    0     0  1341k      0  0:12:01  0:00:03  0:11:58 1341k\r  0  945M    0 7520k    0     0  1579k      0  0:10:12  0:00:04  0:10:08 1578k\r  1  945M    1 10.0M    0     0  1774k      0  0:09:05  0:00:05  0:09:00 2070k\r  1  945M    1 12.5M    0     0  1906k      0  0:08:27  0:00:06  0:08:21 2551k\r  1  945M    1 15.6M    0     0  2065k      0  0:07:48  0:00:07  0:07:41 2723k\r  1  945M    1 18.5M    0     0  2165k      0  0:07:26  0:00:08  0:07:18 2792k\r  2  945M    2 21.8M    0     0  2298k      0  0:07:01  0:00:09  0:06:52 2985k\r  2  945M    2 26.1M    0     0  2483k      0  0:06:29  0:00:10  0:06:19 3309k\r  3  945M    3 30.6M    0     0  2667k      0  0:06:02  0:00:11  0:05:51 3692k\r  3  945M    3 36.6M    0     0  2944k      0  0:05:28  0:00:12  0:05:16 4304k\r  4  945M    4 42.9M    0     0  3196k      0  0:05:02  0:00:13  0:04:49 5012k\r  5  945M    5 51.5M    0     0  3579k      0  0:04:30  0:00:14  0:04:16 6073k\r  6  945M    6 60.5M    0     0  3933k      0  0:04:06  0:00:15  0:03:51 7077k\r  7  945M    7 71.2M    0     0  4361k      0  0:03:41  0:00:16  0:03:25 8363k\r  9  945M    9 85.6M    0     0  4942k      0  0:03:15  0:00:17  0:02:58  9.8M\r 10  945M   10  100M    0     0  5471k      0  0:02:56  0:00:18  0:02:38 11.4M\r 12  945M   12  117M    0     0  6101k      0  0:02:38  0:00:19  0:02:19 13.2M\r 14  945M   14  136M    0     0  6718k      0  0:02:24  0:00:20  0:02:04 15.1M\r 16  945M   16  154M    0     0  7270k      0  0:02:13  0:00:21  0:01:52 16.6M\r 18  945M   18  172M    0     0  7766k      0  0:02:04  0:00:22  0:01:42 17.3M\r 20  945M   20  191M    0     0  8259k      0  0:01:57  0:00:23  0:01:34 18.3M\r 22  945M   22  209M    0     0  8680k      0  0:01:51  0:00:24  0:01:27 18.4M\r 24  945M   24  227M    0     0  9063k      0  0:01:46  0:00:25  0:01:21 18.3M\r 26  945M   26  246M    0     0  9428k      0  0:01:42  0:00:26  0:01:16 18.3M\r 27  945M   27  264M    0     0  9747k      0  0:01:39  0:00:27  0:01:12 18.2M\r 29  945M   29  282M    0     0   9.8M      0  0:01:36  0:00:28  0:01:08 18.1M\r 31  945M   31  300M    0     0  10.1M      0  0:01:33  0:00:29  0:01:04 18.1M\r 33  945M   33  318M    0     0  10.3M      0  0:01:31  0:00:30  0:01:01 18.1M\r 35  945M   35  336M    0     0  10.5M      0  0:01:29  0:00:31  0:00:58 17.9M\r 37  945M   37  354M    0     0  10.8M      0  0:01:27  0:00:32  0:00:55 18.1M\r 39  945M   39  372M    0     0  11.0M      0  0:01:25  0:00:33  0:00:52 18.0M\r 41  945M   41  391M    0     0  11.2M      0  0:01:23  0:00:34  0:00:49 18.1M\r 43  945M   43  409M    0     0  11.4M      0  0:01:22  0:00:35  0:00:47 18.1M\r 45  945M   45  427M    0     0  11.6M      0  0:01:21  0:00:36  0:00:45 18.2M\r 47  945M   47  446M    0     0  11.8M      0  0:01:19  0:00:37  0:00:42 18.2M\r 49  945M   49  463M    0     0  11.9M      0  0:01:19  0:00:38  0:00:41 18.2M\r 50  945M   50  481M    0     0  12.1M      0  0:01:17  0:00:39  0:00:38 18.1M\r 52  945M   52  500M    0     0  12.2M      0  0:01:16  0:00:40  0:00:36 18.2M\r 54  945M   54  517M    0     0  12.4M      0  0:01:16  0:00:41  0:00:35 18.1M\r 56  945M   56  535M    0     0  12.5M      0  0:01:15  0:00:42  0:00:33 17.9M\r 58  945M   58  554M    0     0  12.6M      0  0:01:14  0:00:43  0:00:31 18.2M\r 60  945M   60  572M    0     0  12.8M      0  0:01:13  0:00:44  0:00:29 18.1M\r 62  945M   62  590M    0     0  12.9M      0  0:01:13  0:00:45  0:00:28 18.0M\r 64  945M   64  606M    0     0  12.9M      0  0:01:13  0:00:46  0:00:27 17.3M\r 65  945M   65  621M    0     0  13.0M      0  0:01:12  0:00:47  0:00:25 17.0M\r 67  945M   67  638M    0     0  13.1M      0  0:01:12  0:00:48  0:00:24 16.7M\r 69  945M   69  655M    0     0  13.1M      0  0:01:11  0:00:49  0:00:22 16.5M\r 71  945M   71  672M    0     0  13.2M      0  0:01:11  0:00:50  0:00:21 16.4M\r 73  945M   73  690M    0     0  13.3M      0  0:01:10  0:00:51  0:00:19 17.1M\r 74  945M   74  708M    0     0  13.4M      0  0:01:10  0:00:52  0:00:18 17.5M\r 76  945M   76  726M    0     0  13.5M      0  0:01:09  0:00:53  0:00:16 17.5M\r 78  945M   78  745M    0     0  13.6M      0  0:01:09  0:00:54  0:00:15 17.9M\r 80  945M   80  762M    0     0  13.6M      0  0:01:09  0:00:55  0:00:14 18.0M\r 82  945M   82  780M    0     0  13.7M      0  0:01:08  0:00:56  0:00:12 17.9M\r 84  945M   84  798M    0     0  13.8M      0  0:01:08  0:00:57  0:00:11 17.9M\r 86  945M   86  816M    0     0  13.9M      0  0:01:07  0:00:58  0:00:09 18.0M\r 88  945M   88  835M    0     0  13.9M      0  0:01:07  0:00:59  0:00:08 17.9M\r 90  945M   90  853M    0     0  14.0M      0  0:01:07  0:01:00  0:00:07 18.0M\r 92  945M   92  871M    0     0  14.1M      0  0:01:06  0:01:01  0:00:05 18.1M\r 94  945M   94  889M    0     0  14.1M      0  0:01:06  0:01:02  0:00:04 18.2M\r 96  945M   96  907M    0     0  14.2M      0  0:01:06  0:01:03  0:00:03 18.2M\r 97  945M   97  925M    0     0  14.2M      0  0:01:06  0:01:04  0:00:02 18.1M\r 99  945M   99  943M    0     0  14.3M      0  0:01:05  0:01:05 --:--:-- 18.0M\r100  945M  100  945M    0     0  14.3M      0  0:01:05  0:01:05 --:--:-- 18.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  973M    0 16384    0     0  12525      0 22:37:59  0:00:01 22:37:58 12516\r  0  973M    0 1584k    0     0   714k      0  0:23:15  0:00:02  0:23:13  713k\r  1  973M    1 16.7M    0     0  5378k      0  0:03:05  0:00:03  0:03:02 5376k\r  3  973M    3 31.9M    0     0  7814k      0  0:02:07  0:00:04  0:02:03 7812k\r  4  973M    4 48.6M    0     0  9631k      0  0:01:43  0:00:05  0:01:38  9.8M\r  6  973M    6 64.1M    0     0  10.3M      0  0:01:33  0:00:06  0:01:27 13.1M\r  8  973M    8 80.4M    0     0  11.2M      0  0:01:26  0:00:07  0:01:19 15.9M\r  9  973M    9 96.5M    0     0  11.8M      0  0:01:22  0:00:08  0:01:14 15.9M\r 11  973M   11  112M    0     0  12.2M      0  0:01:19  0:00:09  0:01:10 16.1M\r 13  973M   13  129M    0     0  12.7M      0  0:01:16  0:00:10  0:01:06 16.1M\r 15  973M   15  146M    0     0  13.0M      0  0:01:14  0:00:11  0:01:03 16.4M\r 16  973M   16  164M    0     0  13.4M      0  0:01:12  0:00:12  0:01:00 16.7M\r 18  973M   18  182M    0     0  13.8M      0  0:01:10  0:00:13  0:00:57 17.1M\r 20  973M   20  199M    0     0  14.1M      0  0:01:09  0:00:14  0:00:55 17.4M\r 21  973M   21  213M    0     0  14.0M      0  0:01:09  0:00:15  0:00:54 16.6M\r 23  973M   23  228M    0     0  14.1M      0  0:01:08  0:00:16  0:00:52 16.4M\r 25  973M   25  246M    0     0  14.3M      0  0:01:07  0:00:17  0:00:50 16.4M\r 26  973M   26  260M    0     0  14.3M      0  0:01:07  0:00:18  0:00:49 15.6M\r 28  973M   28  276M    0     0  14.3M      0  0:01:07  0:00:19  0:00:48 15.2M\r 29  973M   29  291M    0     0  14.4M      0  0:01:07  0:00:20  0:00:47 15.5M\r 31  973M   31  307M    0     0  14.5M      0  0:01:07  0:00:21  0:00:46 15.7M\r 33  973M   33  322M    0     0  14.5M      0  0:01:06  0:00:22  0:00:44 15.1M\r 34  973M   34  340M    0     0  14.6M      0  0:01:06  0:00:23  0:00:43 15.9M\r 36  973M   36  359M    0     0  14.8M      0  0:01:05  0:00:24  0:00:41 16.6M\r 38  973M   38  377M    0     0  15.0M      0  0:01:04  0:00:25  0:00:39 17.3M\r 40  973M   40  396M    0     0  15.1M      0  0:01:04  0:00:26  0:00:38 17.7M\r 42  973M   42  414M    0     0  15.2M      0  0:01:03  0:00:27  0:00:36 18.5M\r 44  973M   44  432M    0     0  15.3M      0  0:01:03  0:00:28  0:00:35 18.3M\r 45  973M   45  443M    0     0  15.2M      0  0:01:04  0:00:29  0:00:35 16.8M\r 47  973M   47  460M    0     0  15.2M      0  0:01:03  0:00:30  0:00:33 16.5M\r 49  973M   49  477M    0     0  15.3M      0  0:01:03  0:00:31  0:00:32 16.1M\r 50  973M   50  493M    0     0  15.3M      0  0:01:03  0:00:32  0:00:31 15.7M\r 52  973M   52  510M    0     0  15.3M      0  0:01:03  0:00:33  0:00:30 15.6M\r 54  973M   54  526M    0     0  15.3M      0  0:01:03  0:00:34  0:00:29 16.4M\r 55  973M   55  541M    0     0  15.4M      0  0:01:03  0:00:35  0:00:28 16.3M\r 57  973M   57  558M    0     0  15.4M      0  0:01:03  0:00:36  0:00:27 16.2M\r 59  973M   59  575M    0     0  15.4M      0  0:01:02  0:00:37  0:00:25 16.3M\r 60  973M   60  592M    0     0  15.5M      0  0:01:02  0:00:38  0:00:24 16.4M\r 62  973M   62  609M    0     0  15.5M      0  0:01:02  0:00:39  0:00:23 16.7M\r 64  973M   64  626M    0     0  15.5M      0  0:01:02  0:00:40  0:00:22 16.8M\r 65  973M   65  641M    0     0  15.5M      0  0:01:02  0:00:41  0:00:21 16.5M\r 67  973M   67  658M    0     0  15.6M      0  0:01:02  0:00:42  0:00:20 16.5M\r 69  973M   69  675M    0     0  15.6M      0  0:01:02  0:00:43  0:00:19 16.4M\r 71  973M   71  691M    0     0  15.6M      0  0:01:02  0:00:44  0:00:18 16.3M\r 72  973M   72  707M    0     0  15.6M      0  0:01:02  0:00:45  0:00:17 16.1M\r 74  973M   74  722M    0     0  15.6M      0  0:01:02  0:00:46  0:00:16 16.0M\r 75  973M   75  738M    0     0  15.6M      0  0:01:02  0:00:47  0:00:15 16.1M\r 77  973M   77  754M    0     0  15.6M      0  0:01:02  0:00:48  0:00:14 15.9M\r 79  973M   79  770M    0     0  15.6M      0  0:01:02  0:00:49  0:00:13 15.8M\r 80  973M   80  786M    0     0  15.6M      0  0:01:02  0:00:50  0:00:12 15.9M\r 82  973M   82  803M    0     0  15.6M      0  0:01:01  0:00:51  0:00:10 16.2M\r 84  973M   84  820M    0     0  15.7M      0  0:01:01  0:00:52  0:00:09 16.2M\r 85  973M   85  834M    0     0  15.6M      0  0:01:02  0:00:53  0:00:09 15.9M\r 87  973M   87  852M    0     0  15.7M      0  0:01:01  0:00:54  0:00:07 16.4M\r 89  973M   89  867M    0     0  15.7M      0  0:01:01  0:00:55  0:00:06 16.1M\r 90  973M   90  883M    0     0  15.7M      0  0:01:01  0:00:56  0:00:05 16.0M\r 92  973M   92  899M    0     0  15.7M      0  0:01:01  0:00:57  0:00:04 15.9M\r 94  973M   94  917M    0     0  15.7M      0  0:01:01  0:00:58  0:00:03 16.5M\r 95  973M   95  933M    0     0  15.7M      0  0:01:01  0:00:59  0:00:02 16.2M\r 97  973M   97  950M    0     0  15.7M      0  0:01:01  0:01:00  0:00:01 16.6M\r 99  973M   99  967M    0     0  15.8M      0  0:01:01  0:01:01 --:--:-- 16.6M\r100  973M  100  973M    0     0  15.8M      0  0:01:01  0:01:01 --:--:-- 16.8M\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\n",
            "ERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.7.1+cu101 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvYnr-4JehYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3ae755-000b-43f7-985f-79cfca5b6880"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhSJpxLVxqkm"
      },
      "source": [
        "import argparse\n",
        "import gzip\n",
        "import json\n",
        "import logging\n",
        "import mlflow\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm, trange\n",
        "from IPython import get_ipython\n",
        "from pyngrok import ngrok\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s: %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSZm2RKlx20b"
      },
      "source": [
        "class MeliChallengeDataset(IterableDataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path,\n",
        "                 random_buffer_size=2048):\n",
        "        assert random_buffer_size > 0\n",
        "        self.dataset_path = dataset_path\n",
        "        self.random_buffer_size = random_buffer_size\n",
        "\n",
        "        with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
        "            item = json.loads(next(dataset).strip())\n",
        "            self.n_labels = item[\"n_labels\"]\n",
        "            self.dataset_size = item[\"size\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        try:\n",
        "            with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
        "                shuffle_buffer = []\n",
        "\n",
        "                for line in dataset:\n",
        "                    item = json.loads(line.strip())\n",
        "                    item = {\n",
        "                        \"data\": item[\"data\"],\n",
        "                        \"target\": item[\"target\"]\n",
        "                    }\n",
        "\n",
        "                    if self.random_buffer_size == 1:\n",
        "                        yield item\n",
        "                    else:\n",
        "                        shuffle_buffer.append(item)\n",
        "\n",
        "                        if len(shuffle_buffer) == self.random_buffer_size:\n",
        "                            random.shuffle(shuffle_buffer)\n",
        "                            for item in shuffle_buffer:\n",
        "                                yield item\n",
        "                            shuffle_buffer = []\n",
        "\n",
        "                if len(shuffle_buffer) > 0:\n",
        "                    random.shuffle(shuffle_buffer)\n",
        "                    for item in shuffle_buffer:\n",
        "                        yield item\n",
        "        except GeneratorExit:\n",
        "            return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_X0XsyaIdoR"
      },
      "source": [
        "common_params = {\n",
        "    'train_data': \"./data/meli-challenge-2019/spanish.train.jsonl.gz\",\n",
        "    'token_to_index': \"./data/meli-challenge-2019/spanish_token_to_index.json.gz\",\n",
        "    'pretrained_embeddings': \"./data/SBW-vectors-300-min5.txt.gz\",\n",
        "    'language': \"spanish\",\n",
        "    'test_data': None, # \"./data/meli-challenge-2019/spanish.test.jsonl.gz\",\n",
        "    'validation_data': \"./data/meli-challenge-2019/spanish.validation.jsonl.gz\",\n",
        "}\n",
        "\n",
        "parametrizable_params = [\n",
        "  {\n",
        "    'embeddings_size': 300,\n",
        "    'epochs': 3,\n",
        "    'act_fun': 1,\n",
        "    'FILTERS_COUNT': 100,\n",
        "    'FILTERS_LENGTH': [2, 3, 4]\n",
        "  },\n",
        "\n",
        "  {\n",
        "    'embeddings_size': 300,\n",
        "    'epochs': 3,\n",
        "    'act_fun': 2,\n",
        "    'FILTERS_COUNT': 100,\n",
        "    'FILTERS_LENGTH': [4, 5, 6]\n",
        "  },\n",
        "\n",
        "  #   'embeddings_size': 300,\n",
        "  #   'hidden_layers': [1024, 512, 256, 128], \n",
        "  #   'dropout': 0.2,\n",
        "  #   'epochs': 3,\n",
        "  #   'act_fun': 2,\n",
        "  # }\n",
        "]\n",
        "\n",
        "# cambiar: epochs = 5, 7, 10\n",
        "# cambiar: FILTERS_COUNT\n",
        "# cambiar: FILTERS_LENGTH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMuOEZfNx3un"
      },
      "source": [
        "class PadSequences:\n",
        "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
        "        assert max_length is None or min_length <= max_length\n",
        "        self.pad_value = pad_value\n",
        "        self.max_length = max_length\n",
        "        self.min_length = min_length\n",
        "\n",
        "    def __call__(self, items):\n",
        "        data = [item[\"data\"] for item in items]\n",
        "        target = [item[\"target\"] for item in items]\n",
        "        seq_lengths = [len(d) for d in data]\n",
        "\n",
        "        if self.max_length:\n",
        "            max_length = self.max_length\n",
        "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
        "        else:\n",
        "            max_length = max(self.min_length, max(seq_lengths))\n",
        "\n",
        "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
        "                for d, l in zip(data, seq_lengths)]\n",
        "\n",
        "        return {\n",
        "            \"data\": torch.LongTensor(data),\n",
        "            \"target\": torch.LongTensor(target)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCTJt0yGx-G8"
      },
      "source": [
        "# class MLPClassifier(nn.Module):\n",
        "#     # Pytorch Module\n",
        "#     # __init__:defines the structure of the network\n",
        "#     def __init__(self,\n",
        "#                  pretrained_embeddings_path,\n",
        "#                  token_to_index,\n",
        "#                  n_labels,\n",
        "#                  hidden_layers=[256, 128],\n",
        "#                  dropout=0.3,\n",
        "#                  vector_size=300,\n",
        "#                  act_fun=1,\n",
        "#                  freeze_embedings=True):\n",
        "#         super().__init__()\n",
        "#         with gzip.open(token_to_index, \"rt\") as fh:\n",
        "#             token_to_index = json.load(fh)\n",
        "#         embeddings_matrix = torch.randn(len(token_to_index), vector_size)\n",
        "#         embeddings_matrix[0] = torch.zeros(vector_size)\n",
        "#         with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
        "#             next(fh)\n",
        "#             for line in fh:\n",
        "#                 word, vector = line.strip().split(None, 1)\n",
        "#                 if word in token_to_index:\n",
        "#                     embeddings_matrix[token_to_index[word]] =\\\n",
        "#                         torch.FloatTensor([float(n) for n in vector.split()])\n",
        "#         self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
        "#                                                        freeze=freeze_embedings,\n",
        "#                                                        padding_idx=0)\n",
        "#         ## Hidden layers definitions\n",
        "#         ############################\n",
        "#         ## https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "#         self.hidden_layers = [\n",
        "#             nn.Linear(vector_size, hidden_layers[0]) # first layer\n",
        "#         ]\n",
        "#         for input_size, output_size in zip(hidden_layers[:-1], hidden_layers[1:]):\n",
        "#             self.hidden_layers.append(\n",
        "#                 nn.Linear(input_size, output_size) # intermediate layers if hidden_layersÂ´s size > 2\n",
        "#             )\n",
        "#         self.dropout = dropout # percentage of disabled neurons\n",
        "#         self.hidden_layers = nn.ModuleList(self.hidden_layers) #  last layer\n",
        "#         self.output = nn.Linear(hidden_layers[-1], n_labels) \n",
        "#         self.vector_size = vector_size\n",
        "#         self.act_fun = act_fun\n",
        "\n",
        "\n",
        "#     ############################\n",
        "#     # forward: defines how the network layers interact\n",
        "#     def forward(self, x):\n",
        "#         x = self.embeddings(x)\n",
        "#         x = torch.mean(x, dim=1)\n",
        "#         for layer in self.hidden_layers:\n",
        "#             if self.act_fun == 1:\n",
        "#                 x = F.relu(layer(x))\n",
        "#             if self.act_fun == 2:\n",
        "#                 x = F.celu(layer(x))\n",
        "#             if self.dropout:\n",
        "#                 x = F.dropout(x, self.dropout)\n",
        "#         x = self.output(x)\n",
        "#         return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ8Hq14oBA41"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 pretrained_embeddings_path, \n",
        "                 token_to_index,             \n",
        "                 n_labels,\n",
        "                 vector_size,\n",
        "                 FILTERS_COUNT,\n",
        "                 FILTERS_LENGTH,\n",
        "                 act_fun,\n",
        "                 freeze_embedings):\n",
        "        super().__init__()\n",
        "        with gzip.open(token_to_index, \"rt\") as fh:\n",
        "            token_to_index = json.load(fh)\n",
        "        embeddings_matrix = torch.randn(len(token_to_index), vector_size)\n",
        "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
        "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
        "            next(fh)\n",
        "            for line in fh:\n",
        "                word, vector = line.strip().split(None, 1)\n",
        "                if word in token_to_index:\n",
        "                    embeddings_matrix[token_to_index[word]] =\\\n",
        "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
        "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
        "                                                       freeze=freeze_embedings,\n",
        "                                                       padding_idx=0)\n",
        "        self.FILTERS_COUNT = FILTERS_COUNT\n",
        "        self.FILTERS_LENGTH = FILTERS_LENGTH\n",
        "        self.act_fun = act_fun\n",
        "        self.convs = []\n",
        "        for filter_lenght in self.FILTERS_LENGTH:\n",
        "            self.convs.append(\n",
        "                nn.Conv1d(vector_size, self.FILTERS_COUNT, filter_lenght)\n",
        "            )\n",
        "        self.convs = nn.ModuleList(self.convs)\n",
        "        self.fc = nn.Linear(self.FILTERS_COUNT * len(self.FILTERS_LENGTH), 128)\n",
        "        self.output = nn.Linear(128, n_labels)\n",
        "        self.vector_size = vector_size\n",
        "    \n",
        "    @staticmethod\n",
        "    def conv_global_max_pool(x, conv):\n",
        "        return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x).transpose(1, 2)\n",
        "        x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
        "        x = torch.cat(x, dim=1)\n",
        "        if self.act_fun == 1:\n",
        "            x = F.relu(self.fc(x))\n",
        "        if self.act_fun == 2:\n",
        "            x = F.celu(self.fc(x))\n",
        "        # cambiar: x = F.hardsigmoid(layer(x)); F.celu(layer(x)); ; F.leaky_relu(layer(x))         \n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB6Tql2pd4zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c28dbe0-d40f-4bb2-e76d-be101aed639c"
      },
      "source": [
        "pad_sequences = PadSequences(\n",
        "    pad_value=0,\n",
        "    max_length=None,\n",
        "    min_length=1\n",
        ")\n",
        "\n",
        "logging.info(\"Building training dataset\")\n",
        "# An iterable Dataset.\n",
        "# All datasets that represent an iterable of data samples should subclass it. \n",
        "# Such form of datasets is particularly useful when data come from a stream.\n",
        "# All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.\n",
        "train_dataset = MeliChallengeDataset(\n",
        "    dataset_path=common_params.get('train_data'),\n",
        "    random_buffer_size=2048  # This can be a hypterparameter\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,              # dataset from which to load the data.\n",
        "    batch_size=128,             # This can be a hyperparameter # how many samples per batch to load (default: ``1``).\n",
        "    shuffle=False,              # set to ``True`` to have the data reshuffled at every epoch (default: ``False``).\n",
        "    collate_fn=pad_sequences,   # merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.\n",
        "    drop_last=False,             # set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. \n",
        "                                # If ``False`` and the size of dataset is not divisible by the batch size, then the last batch\n",
        "                                # will be smaller. (default: ``False``)\n",
        "    num_workers=2             # how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. (default: ``0``)\n",
        ")\n",
        "\n",
        "if common_params.get('validation_data'):\n",
        "     logging.info(\"Building validation dataset\")\n",
        "     validation_dataset = MeliChallengeDataset(\n",
        "         dataset_path=common_params.get('validation_data'),\n",
        "         random_buffer_size=1\n",
        "     )\n",
        "     validation_loader = DataLoader(\n",
        "         validation_dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=False,\n",
        "         collate_fn=pad_sequences,\n",
        "         drop_last=False\n",
        "     )\n",
        "else:\n",
        "     validation_dataset = None\n",
        "     validation_loader = None\n",
        "\n",
        "if common_params.get('test_data'):\n",
        "     logging.info(\"Building test dataset\")\n",
        "     test_dataset = MeliChallengeDataset(\n",
        "         dataset_path=common_params.get('test_data'),\n",
        "         random_buffer_size=1\n",
        "     )\n",
        "     test_loader = DataLoader(\n",
        "         test_dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=False,\n",
        "         collate_fn=pad_sequences,\n",
        "         drop_last=False\n",
        "     )\n",
        "else:\n",
        "    test_dataset = None\n",
        "    test_loader = None\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 22:46:06,841: INFO - Building training dataset\n",
            "2021-03-21 22:46:06,848: INFO - Building validation dataset\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqJXrlGQavLN"
      },
      "source": [
        "## Iterando params\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U46ly2RPVf0R"
      },
      "source": [
        "# for params in parametrizable_params:\n",
        "#   mlflow.set_experiment(f\"diplodatos.{common_params.get('language')}\")\n",
        "#   with mlflow.start_run():\n",
        "#     logging.info(\"Starting experiment\")\n",
        "#     # Log all relevent hyperparameters\n",
        "#     mlflow.log_params({\n",
        "#       \"model_type\": \"Multilayer Perceptron\",\n",
        "#       \"embeddings\": common_params.get('pretrained_embeddings'),\n",
        "#       **params\n",
        "#     })\n",
        "#     device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#     logging.info(\"Building classifier\")\n",
        "#     model = MLPClassifier(\n",
        "#         pretrained_embeddings_path=common_params.get('pretrained_embeddings'),\n",
        "#         token_to_index=common_params.get('token_to_index'),\n",
        "#         n_labels=train_dataset.n_labels,\n",
        "#         hidden_layers=params.get('hidden_layers'),\n",
        "#         dropout=params.get('dropout'),\n",
        "#         vector_size=params.get('embeddings_size'),\n",
        "#         act_fun=params.get('act_fun'),\n",
        "#         freeze_embedings=True  # This can be a hyperparameter\n",
        "#     )\n",
        "#     model = model.to(device)\n",
        "#     # loss function\n",
        "#     # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#     loss = nn.CrossEntropyLoss()        \n",
        "#     # optimizer algorithm\n",
        "#     # https://pytorch.org/docs/stable/optim.html\n",
        "#     # cambiar: lr; weight_decay; momentum\n",
        "#     optimizer = optim.Adam(\n",
        "#         model.parameters(),\n",
        "#         lr=1e-3,           # This can be a hyperparameter\n",
        "#         weight_decay=1e-5  # This can be a hyperparameter # weight for L2 regularization\n",
        "#         # momentum=        # This can be a hyperparameter\n",
        "#     )\n",
        "\n",
        "#     logging.info(\"Training classifier\")\n",
        "#     for epoch in trange(params.get('epochs')):\n",
        "#         model.train()\n",
        "#         running_loss = []\n",
        "#         for idx, batch in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
        "#             # set to zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # get the inputs; data and target\n",
        "#             data = batch[\"data\"].to(device)\n",
        "#             target = batch[\"target\"].to(device)\n",
        "#             # forward + backward + optimize\n",
        "#             output = model(data) # MLPClassifier\n",
        "#             loss_value = loss(output, target)\n",
        "#             loss_value.backward()\n",
        "#             optimizer.step()\n",
        "#             # statistics\n",
        "#             running_loss.append(loss_value.item())\n",
        "#         mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "\n",
        "#         if validation_dataset:\n",
        "#             logging.info(\"Evaluating model on validation\")\n",
        "#             model.eval()\n",
        "#             running_loss = []\n",
        "#             targets = []\n",
        "#             predictions = []\n",
        "#             with torch.no_grad():\n",
        "#                 for batch in tqdm(validation_loader, position=0, leave=True):\n",
        "#                     data = batch[\"data\"].to(device)\n",
        "#                     target = batch[\"target\"].to(device)\n",
        "#                     output = model(data)\n",
        "#                     running_loss.append(\n",
        "#                         loss(output, target).item()\n",
        "#                     )\n",
        "#                     targets.extend(batch[\"target\"].numpy())\n",
        "#                     predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "#                 mlflow.log_metric(\"validation_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "#                 mlflow.log_metric(\"validation_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
        "\n",
        "#     if test_dataset:\n",
        "#         logging.info(\"Evaluating model on test\")\n",
        "#         model.eval()\n",
        "#         running_loss = []\n",
        "#         targets = []\n",
        "#         predictions = []\n",
        "#         with torch.no_grad():\n",
        "#             for batch in tqdm(test_loader, position=0, leave=True):\n",
        "#                 data = batch[\"data\"].to(device)\n",
        "#                 target = batch[\"target\"].to(device)\n",
        "#                 output = model(data)\n",
        "#                 running_loss.append(\n",
        "#                     loss(output, target).item()\n",
        "#                 )\n",
        "#                 targets.extend(batch[\"target\"].numpy())\n",
        "#                 predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "#             mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "#             mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNkOoav0w8eU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c969f5e-7f2c-4310-8de1-84b2cf44fa7f"
      },
      "source": [
        "for params in parametrizable_params:\n",
        "  mlflow.set_experiment(f\"diplodatos.{common_params.get('language')}\")\n",
        "  with mlflow.start_run():\n",
        "    logging.info(\"Starting experiment\")\n",
        "    # Log all relevent hyperparameters\n",
        "    mlflow.log_params({\n",
        "      \"model_type\": \"Convolutional Neural Network\",\n",
        "      \"embeddings\": common_params.get('pretrained_embeddings'),\n",
        "      **params\n",
        "    })\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    logging.info(\"Building classifier\")\n",
        "    model = CNN(\n",
        "        pretrained_embeddings_path=common_params.get('pretrained_embeddings'),\n",
        "        token_to_index=common_params.get('token_to_index'),\n",
        "        n_labels=train_dataset.n_labels,\n",
        "        vector_size=params.get('embeddings_size'),\n",
        "        FILTERS_COUNT=params.get('FILTERS_COUNT'),\n",
        "        FILTERS_LENGTH=params.get('FILTERS_LENGTH'),\n",
        "        act_fun=params.get('act_fun'),\n",
        "        freeze_embedings=True  # This can be a hyperparameter\n",
        "    )\n",
        "     \n",
        "    model = model.to(device)\n",
        "    # loss function\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    loss = nn.CrossEntropyLoss()        \n",
        "    # optimizer algorithm\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    # cambiar: lr; weight_decay; momentum\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,           # This can be a hyperparameter\n",
        "        weight_decay=1e-5  # This can be a hyperparameter # weight for L2 regularization\n",
        "        # momentum=        # This can be a hyperparameter\n",
        "    )\n",
        "\n",
        "    logging.info(\"Training classifier\")\n",
        "    for epoch in trange(params.get('epochs')):\n",
        "        model.train()\n",
        "        running_loss = []\n",
        "        for idx, batch in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
        "            # set to zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # get the inputs; data and target\n",
        "            data = batch[\"data\"].to(device)\n",
        "            target = batch[\"target\"].to(device)\n",
        "            # forward + backward + optimize\n",
        "            output = model(data) # MLPClassifier\n",
        "            loss_value = loss(output, target)\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            # statistics\n",
        "            running_loss.append(loss_value.item())\n",
        "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "\n",
        "        if validation_dataset:\n",
        "            logging.info(\"Evaluating model on validation\")\n",
        "            model.eval()\n",
        "            running_loss = []\n",
        "            targets = []\n",
        "            predictions = []\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(validation_loader, position=0, leave=True):\n",
        "                    data = batch[\"data\"].to(device)\n",
        "                    target = batch[\"target\"].to(device)\n",
        "                    output = model(data)\n",
        "                    running_loss.append(\n",
        "                        loss(output, target).item()\n",
        "                    )\n",
        "                    targets.extend(batch[\"target\"].numpy())\n",
        "                    predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "                mlflow.log_metric(\"validation_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "                mlflow.log_metric(\"validation_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
        "\n",
        "    if test_dataset:\n",
        "        logging.info(\"Evaluating model on test\")\n",
        "        model.eval()\n",
        "        running_loss = []\n",
        "        targets = []\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, position=0, leave=True):\n",
        "                data = batch[\"data\"].to(device)\n",
        "                target = batch[\"target\"].to(device)\n",
        "                output = model(data)\n",
        "                running_loss.append(\n",
        "                    loss(output, target).item()\n",
        "                )\n",
        "                targets.extend(batch[\"target\"].numpy())\n",
        "                predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
        "            mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
        "            mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 22:46:07,025: INFO - Starting experiment\n",
            "2021-03-21 22:46:07,096: INFO - Building classifier\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO: 'diplodatos.spanish' does not exist. Creating a new experiment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 22:46:39,039: INFO - Training classifier\n",
            "76490it [07:40, 166.14it/s]\n",
            "2021-03-21 22:54:19,453: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:25<00:00, 376.91it/s]\n",
            "76490it [07:49, 162.79it/s]\n",
            "2021-03-21 23:02:36,427: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:25<00:00, 377.78it/s]\n",
            "76490it [07:47, 163.46it/s]\n",
            "2021-03-21 23:10:51,536: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:25<00:00, 376.88it/s]\n",
            "100%|ââââââââââ| 3/3 [24:39<00:00, 493.24s/it]\n",
            "2021-03-21 23:11:18,773: INFO - Starting experiment\n",
            "2021-03-21 23:11:18,778: INFO - Building classifier\n",
            "2021-03-21 23:11:43,534: INFO - Training classifier\n",
            "76490it [07:51, 162.27it/s]\n",
            "2021-03-21 23:19:34,938: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:26<00:00, 362.24it/s]\n",
            "76490it [07:52, 162.05it/s]\n",
            "2021-03-21 23:27:55,210: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:26<00:00, 363.97it/s]\n",
            "76490it [07:50, 162.41it/s]\n",
            "2021-03-21 23:36:14,310: INFO - Evaluating model on validation\n",
            "100%|ââââââââââ| 9562/9562 [00:26<00:00, 363.50it/s]\n",
            "100%|ââââââââââ| 3/3 [24:58<00:00, 499.64s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKw8EKRXgPzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e099eaad-1a36-494f-ff9e-d7a1a6e8ba28"
      },
      "source": [
        "    # run tracking UI in the background\n",
        "    get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n",
        "\n",
        "\n",
        "    # create remote tunnel using ngrok.com to allow local port access\n",
        "    # borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "\n",
        "    # Terminate open tunnels if exist\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Setting the authtoken (optional)\n",
        "    # Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "    NGROK_AUTH_TOKEN = \"\"\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    # Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "    ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "    print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 23:36:44,906: INFO - Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.7/dist-packages/pyngrok/bin/ngrok\n",
            "2021-03-21 23:36:45,018: INFO - Opening tunnel named: http-5000-3205da49-0113-4b97-85a4-2533774620e9\n",
            "2021-03-21 23:36:45,113: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2021-03-21 23:36:45,119: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2021-03-21 23:36:45,120: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2021-03-21 23:36:45,124: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2021-03-21 23:36:45,203: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2021-03-21 23:36:45,206: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"client session established\" obj=csess id=bc84c5ba9214\n",
            "2021-03-21 23:36:45,218: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=start pg=/api/tunnels id=2c15c143e95eb057\n",
            "2021-03-21 23:36:45,221: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=end pg=/api/tunnels id=2c15c143e95eb057 status=200 dur=452.859Âµs\n",
            "2021-03-21 23:36:45,224: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=start pg=/api/tunnels id=dc0f78dd3b70ded9\n",
            "2021-03-21 23:36:45,226: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=end pg=/api/tunnels id=dc0f78dd3b70ded9 status=200 dur=95.368Âµs\n",
            "2021-03-21 23:36:45,227: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=start pg=/api/tunnels id=0f31324f3a09a3a6\n",
            "2021-03-21 23:36:45,236: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-5000-3205da49-0113-4b97-85a4-2533774620e9 addr=http://localhost:5000 url=https://37977c8b320d.ngrok.io\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://37977c8b320d.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-21 23:36:45,239: INFO - t=2021-03-21T23:36:45+0000 lvl=info msg=end pg=/api/tunnels id=0f31324f3a09a3a6 status=201 dur=18.213516ms\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvCZfkCEgcIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5495cd0d-cac7-4f0d-e74f-1e32eb104510"
      },
      "source": [
        "!zip -r ./mlruns_cnn.zip ./mlruns\n",
        "from google.colab import files\n",
        "# files.download(\"./mlruns.zip\")\n",
        "!cp ./mlruns_cnn.zip ./drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: mlruns/ (stored 0%)\n",
            "  adding: mlruns/.trash/ (stored 0%)\n",
            "  adding: mlruns/1/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/metrics/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/metrics/validation_bacc (deflated 34%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/metrics/train_loss (deflated 32%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/metrics/validation_loss (deflated 33%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/artifacts/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/tags/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/tags/mlflow.source.type (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/tags/mlflow.user (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/tags/mlflow.source.name (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/meta.yaml (deflated 44%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/ (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/FILTERS_COUNT (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/embeddings_size (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/epochs (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/model_type (deflated 7%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/act_fun (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/FILTERS_LENGTH (stored 0%)\n",
            "  adding: mlruns/1/910676313f24471daf560b29a10fc983/params/embeddings (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/ (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/metrics/ (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/metrics/validation_bacc (deflated 34%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/metrics/train_loss (deflated 35%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/metrics/validation_loss (deflated 33%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/artifacts/ (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/tags/ (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/tags/mlflow.source.type (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/tags/mlflow.user (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/tags/mlflow.source.name (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/meta.yaml (deflated 45%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/ (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/FILTERS_COUNT (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/embeddings_size (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/epochs (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/model_type (deflated 7%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/act_fun (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/FILTERS_LENGTH (stored 0%)\n",
            "  adding: mlruns/1/e09539f5a5e34212953139666b519fea/params/embeddings (stored 0%)\n",
            "  adding: mlruns/1/meta.yaml (deflated 14%)\n",
            "  adding: mlruns/0/ (stored 0%)\n",
            "  adding: mlruns/0/meta.yaml (deflated 11%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsoUsQ7i6eG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}